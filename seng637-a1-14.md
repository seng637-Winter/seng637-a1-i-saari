>   **SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 14      |
|-----------------|
| Ian                |   
| Paul             |   
| Marie              |   
| Matjaz               |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	](#Instruction)

[2 High-level description of the exploratory testing plan	](#High-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing	](#Comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports	](#Notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was divided	](#How-the-pair-testing-was-managed-and-team-work/effort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons learned	](#Difficulties-encountered,-challenges-overcome,-and-lessons-learned)

# Introduction

This lab will cover the team’s efforts to test a simulated ATM interface. The intention is to gain familiarity with exploratory testing, manual functional testing and regression testing. One team member is familiar with bug reporting and testing from their work experience, and will help guide the team’s use of Azure DevOps.


# High-level description of the exploratory testing plan

Scope of Testing
The exploratory testing will cover a wide range of operations mirroring the extent of operations defined in the specification. Tests will follow procedures a normal person might perform when using the ATM. Most ATM functions will be used in the way they are intended to be used. The specification will guide the procedures used in the exploratory tests. While testing the tester will also try any malicious behavior that is thought of in an attempt to “break” the program. This will add to the scope of potential issues that may be encountered. Testing will continue while the testers still have ideas of different ways that the program can be used that have not yet been observed.


Tests will begin by starting the ATM and loading it with $1000. Then the following actions will be performed:
1. Cash withdrawal cash and review account balance
1. Deposit cash, review account balance, and print receipt
1. Attempt to withdraw more cash than held in the account
1. Enter an incorrect PIN
1. Cancel a withdrawal transaction


Features to be tested (with high-level description of intended functionality):
System startup - ATM is turned on and awaits for card to be inserted.


System shutdown - ATM is shut down and must be turned back on in order to be used.


Entering Card and PIN - ATM accepts a card and prompts the user to enter their pin number. If the pin number is correct access to their accounts is granted, if incorrect the user is prompted to re-enter their password. After three incorrect attempts the card is returned to the user.


Withdrawal - The user may withdraw an amount from any account that they own, as long as the amount is available in the chosen account. Confirmation of the transaction is given on the receipt.


Deposit - The user may deposit cash into any account that they own. Confirmation of the transaction is given on the receipt.


Transfer - The user may transfer money between any of the accounts that they own on the inserted card. Confirmation of the transaction is given on the receipt.


Inquiry - The user may inquire of the total amount and available amount in the account that they have chosen.


Test Types:
This testing process will consist of Unit Testing and Acceptance Testing.
Unit Testing - there is only a single module under test
Acceptance Testing - the system will be tested to ensure it satisfies the specification


Test Teams:
Two teams will perform the exploratory and manual functional testing. Team 1 will be Ian and Marie, Team 2 will be Paul and Matjaz. While testing is in progress one of the pair will be testing the application and observing while the other records any bugs that are apparent. The tester that is not in contact with the program and observing will record all use cases explored while creating work items in Azure when bugs are encountered. Once the tester that is using the program has explored all they can, the roles will switch for another round of testing.


# Comparison of exploratory and manual functional testing

## Benefits:
The exploratory testing revealed more bugs than the manual functional testing, though many would have been found by the MFT if they hadn't already been recorded. During exploratory testing, more edge cases like accidental button presses were tested. The real benefit of exploratory testing comes from finding those edge cases that would not be otherwise found using a structured approach. Although the MFT did not uncover edge cases, it ensured that all of the intended functionality of the program was tested systematically, providing good coverage of expected behavior.


## Tradeoffs:
Exploratory testing did uncover more edge cases, although this did come at a cost. Functionality can be overlooked due to this lack of structure, making exploratory testing less reliable for thorough coverage. MF test cases did not find many edge cases since the manual functional test cases were not very extensive and focused primarily on specified functionality. MF tests also exhibited a lack of intentional deviation from predicted user behavior which further limited its ability to uncover unexpected bugs.




## Effectiveness:
Exploratory testing was very effective at finding bugs from unexpected user behavior. This made it very useful in finding hidden bugs and usability problems. MFT was highly effective in testing specific functionality to ensure that all intended user behavior was tested.




## Efficiency
Exploratory testing was by nature less efficient than MFT. Due to its lack of structure, the same bug could be tested over and over again. Exploratory testing included long periods of time essentially performing random functions within the software, this was good for finding unexpected bugs but forced the tester to use the program for long periods of time especially when recording the bugs to ensure that the proper reproduction steps were given. MFT being structured and finite was highly efficient and the tester could very quickly test all of the specified cases. 


In conclusion, each of these testing methodologies brings a very important perspective to the table. Exploratory testing, being unstructured, uncovers unexpected scenarios and issues. MFT, highly structured in nature, gives comprehensive coverage of the core functionality that the program is intended to perform. 


By combining these methodologies, the limitations of each are addressed. MFT tests the specified functionality while Exploratory testing fills in the gaps by exploring unpredictable user behaviour. Together, they create a robust and thorough coverage of all the program's flaws, achieving a more resilient and reliable software product.


# Notes and discussion of the peer reviews of defect reports

Group 1 - Ian and Marie


Group 2 - Paul and Matjaz


Each group conducted independent exploratory, manual scripting, and regression testing. Identified bugs were documented using Azure, following the same general format to ensure reporting consistency. Documentation included details such as the function being tested, the initial state, reproduction steps, the expected outcome, and the actual outcome. After each group completed their independent testing and reporting, a final review meeting was held.


The peer review process helped ensure all bugs were thoroughly documented with clarity and accuracy. Identifying and resolving duplicate bugs involved comparing documentation from both groups, which occasionally posed challenges due to varying approaches in bug description. However, this comparative analysis often led to more comprehensive bug descriptions. Additionally, it was ensured that every identified bug could be replicated following the steps provided in the bug report, confirming their persistence and accurate documentation. This replication process also uncovered additional bugs not identified during initial rounds of testing.


# How the pair testing was managed and team work/effort was divided 

Both teams performed their own exploratory testing, manual functional testing, and regression testing. This was to allow all group members the opportunity to learn the process and the Azure bug-tracking tool. Both teams followed the general exploratory test plan, though the exact inputs varied between groups and thus different bugs were discovered.


# Difficulties encountered, challenges overcome, and lessons learned

The Azure bug tracking tool worked well, once it was configured to accept bug reports.

The biggest challenge encountered was determining if previously entered bug reports covered a new bug. Some bugs could be reproduced by slightly different steps, making them look like new bugs, although the underlying bug was the same. To ensure no potential bugs were ignored, all bug reports were included, though some were noted to be similar to others.


# Comments/feedback

The lab was useful in gaining familiarity with the Azure bug-tracking tool. The instructions were clear, although the formatting of the instruction markdown file was buggy, resulting in misaligned tables. There was also a confusing misalignment between the level of detail of the sample test plan and the instructions for a 'high level test plan'.

